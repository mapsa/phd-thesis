\section{Ridge Regression}
\subsection{Regression problems}
The objective of regression problems is to find a function $f$ which
explains the relation between an input $\mathbf{x}_t \in \R^l$, and an
output $y_t \in \R$ such that: $y_t=f(\mathbf{x}_t) + \epsilon_t$ for
a set of $m$ data points $\{( \mathbf{x}_t,y_t)\}_{t=1}^m$.  If the
relationship between $y_t$ and $\mathbf{x}_t$ is thought to
be linear, $f$ can be written as:

\begin{equation*}
f(\mathbf{x}_t)=\mathbf{w}^\intercal \mathbf{x}_t=\sum_{i=1}^l
w(i)x_t(i) \, ,
\end{equation*}

\noindent where $\mathbf{w}$ is a  weight vector determined in a training phase.

The least squares method is a well known way to solve a regression
problem. This method consists of minimizing the sum of squared error:


\begin{equation}
\label{eq:problem2}
 J(\mathbf{w}) = \sum_{t=1}^m (f(\mathbf{x}_t)-y_t)^2 = \sum_{t=1}^m
 (\mathbf{w}^\intercal {\bf x}_t-y_t)^2 \, ,
\end{equation}

\noindent which is equivalent to minimize $J(\mathbf{w})$ with $\gamma = 0$ in
equation~(\ref{eq:problem}). 
Expressed in matrix form this amounts to: %verificar inglés con profe

\begin{equation*}
J(\mathbf{w})=
	\big\|
       \begin{bmatrix}
       x_1(1) & \cdots & x_1(l) \\
       \vdots & \ddots & \vdots \\
       x_m(1) & \cdots & x_m(l)
       \end{bmatrix}
       \begin{bmatrix}
       w(1)\\
       \vdots \\
       w(l) 
       \end{bmatrix}-
       \begin{bmatrix}
        y_1\\
        \vdots \\
        y_m 
        \end{bmatrix}
       \big\|_{L^2}^2
\end{equation*}  

\noindent or a more reduced expression is:

\begin{equation*}
J(\mathbf{w}) = \| \mathbf{X}\mathbf{w} - \mathbf{y} \|_2^2 ,  
\end{equation*}  

\noindent where

\begin{equation*}
\mathbf{X} = \begin{bmatrix} \mathbf{x}_1^\intercal \\ \vdots \\
\mathbf{x}_m^\intercal \end{bmatrix} , \quad
\mathbf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix} \quad
\text{and}  \quad
\mathbf{w} = \begin{bmatrix} w(1) \\ \vdots \\ w(l) \end{bmatrix}
\end{equation*}
As is well known, the optimal solution $\mathbf{w}_*$ obtained minimizing
equation~(\ref{eq:problem2}) is:

\begin{equation}
\label{eq:MPenrose}
\mathbf{w}_*=(\mathbf{X}^\intercal \mathbf{X})^{-1}\mathbf{X}^\intercal \mathbf{y}
\end{equation}
Thus, when a new input $\mathbf{x}_t$ arrives, the prediction of the target
value $y_t$ is defined as:

\begin{equation*}
f(\mathbf{x}_t)=\mathbf{w}_*^\intercal \mathbf{x}_t \,.
\end{equation*}

\subsection{Ridge Regression}

In order to avoid the singularity of the matrix $\mathbf{X}^\intercal
\mathbf{X}$ in equation~(\ref{eq:MPenrose}), a regularization term is
introduced.  The optimization problem including the regularization
term is shown in equation~(\ref{eq:problem}). 

%\begin{equation*}
%\min_{\mathbf{w}} J(\mathbf{w}), \quad \text{where} \quad J(\mathbf{w}) =
%\sum_{t=1}^m
%(f(\mathbf{x}_t)-y_t)^2  + \gamma R(\mathbf{w}) \, ,
%\end{equation*}
%
%and $\gamma$ is a regularization parameter and term $R(\mathbf{w})$ reduce the
%space of possible solutions simplifying the search in the optimization problem.

When $R(\mathbf{w}) = \| \mathbf{w}\| ^2$, the method is called ridge
regression and the optimal solution $\mathbf{w}_*$ is well known: 

\begin{equation}
\label{eq:optsolRR}
\mathbf{w}_*=(X^\intercal X+\gamma \mathbb{I})^{-1}X^\intercal y \, ,
\end{equation}
Equation~(\ref{eq:optsolRR}) can be also be expressed as:

\begin{eqnarray*}
\mathbf{w}_* =& \displaystyle \big (\sum_{t=1}^m \mathbf{x}_t \mathbf{x}_t  ^\intercal + \gamma
\mathbb{I}\big )^{-1} \sum_{t=1}^m y_t \mathbf{x}_t \\
\mathbf{w}_* =& \mathbf{A}^{-1}\mathbf{b}\, ,
\end{eqnarray*}

\noindent where


\[ 
\mathbf{A}= \sum_{t=1}^m \mathbf{x}_t \mathbf{x}_t  ^\intercal +
\gamma \mathbb{I} 
\qquad \text{and} \qquad 
\mathbf{b}=  \sum_{t=1}^m y_t \mathbf{x}_t 
\, .\]



\begin{algorithm}[H]
\begin{algorithmic}[1]
\REQUIRE $\,$ \\
$\{\mathbf{x}_1,\dots,\mathbf{x}_{m} \}$: $m$ input vectors \\
$\{y_1,\dots,y_{m} \}$: $m$ targets \\
\ENSURE  $\,$ \\
$\{f(\mathbf{x}_1),\dots,f(\mathbf{x}_{m}) \}$: model predictions \\
\STATE Initialize $\mathbf{A}=\gamma \mathbb{I}$
and $\mathbf{b}=0$
\FOR { $t = 1$ to $m$ }
	\STATE read new $\mathbf{x}_t$
	\STATE output prediction $f(\mathbf{x}_t) =  \mathbf{b}^\intercal \mathbf{A}^{-1}\mathbf{x}_t$
   	\STATE $\mathbf{A} = \mathbf{A} + \mathbf{x}_t \mathbf{x}_t^\intercal$
   	\STATE Read new $y_t$
    	\STATE $\mathbf{b} = \mathbf{b} + y_t \mathbf{x}_t$
\ENDFOR

\end{algorithmic}
\caption{Ridge Regression}
\label{alg:RR}
\end{algorithm}

The ridge regression method is shown in algorithm~\ref{alg:RR} where
the prediction value for a new input $\mathbf{x}_{m+1}$ is:

\begin{eqnarray*}
f(\mathbf{x}_{m+1}) =& \mathbf{w}_*^\intercal \mathbf{x}_{m+1} \\
  =& \mathbf{b}^\intercal \mathbf{A}^{-1} \mathbf{x}_{m+1}
\, .
\end{eqnarray*}




%The effect of adding the term $\gamma \mathbb{I}$ to the matrix
%$\mathbf{X}^\intercal \mathbf{X}$ improves 
%its condition number because it increases its diagonal values when $\gamma > 0 $. 
%The matrix $\mathbf{X}^\intercal \mathbf{X}$ is symmetrical
%($(\mathbf{X}^\intercal \mathbf{X})^\intercal = \mathbf{X}^\intercal
%\mathbf{X}$) and is therefore diagonalizable.
%If we now the eigenvalue decomposition of $\mathbf{X}^\intercal \mathbf{X}=\mathbf{VDV}^{-1}$ where the columns of
%$\mathbf{V}$ are its eigenvectors and $\mathbf{D}$ is a diagonal matrix with its eigenvalues,
%then replacing the EVD on equation~(\ref{eq:optsolRR}) we have:
%
%\begin{eqnarray*}
%w_*=&(\mathbf{VDV}^{-1}+ \mathbf{V} \gamma
%\mathbb{I}\mathbf{V}^{-1})^{-1}\mathbf{X}^\intercal \mathbf{Y}\\
%w_*=&(\mathbf{VD}_\gamma \mathbf{V}^{-1})^{-1}\mathbf{X}^\intercal \mathbf{Y} \, ,
%\end{eqnarray*}
%where
%
%\begin{equation*}
%\mathbf{D}_\gamma=
%\begin{bmatrix}
%d_1 + \gamma & \, & \, \\
%\, & d_2 +\gamma & \, \\
%\, & \, & \ddots & \, \\
%\, & \, & \, & d_l +\gamma \, .
%\end{bmatrix}
%\end{equation*}
%where $d_1 \geq d_2 \geq \dots \geq d_l$.
%
%So, the condition number of $\mathbf{A}=\mathbf{X}^\intercal \mathbf{X}+\gamma \mathbb{I}$ is defined as:
%
%\begin{equation*}
%	\kappa = \|\mathbf{A}\| \|\mathbf{A}^{-1}\|
%\end{equation*}
%because $\mathbf{A}$ is simmetrical and therefore non-singular, the condition number can be
%expressed in terms of its eigenvalues:
%
%\begin{eqnarray*}
%\kappa = \frac{\sigma_1}{\sigma_l} \\
%\kappa = \frac{d_1+\gamma}{d_l + \gamma} \,
%\end{eqnarray*}
%which is a better condition number than the problem without regularization
%($\kappa = d_1/d_l$), it means:
%
%\begin{equation*}
%	\frac{d_1+\gamma}{d_l + \gamma} < \frac{d_1}{d_l} \, ,
%\end{equation*}
%for $\gamma > 0$.
%
%
%
%
%
%
%
